{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aisha\\appdata\\roaming\\python\\python36\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from transformers) (2.0.13)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from transformers) (1.19.3)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp36-cp36m-win_amd64.whl\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp36-cp36m-win_amd64.whl (279 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from transformers) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.3.9)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\aisha\\appdata\\roaming\\python\\python36\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\aisha\\appdata\\roaming\\python\\python36\\site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Installing collected packages: regex, packaging, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 16.8\n",
      "    Uninstalling packaging-16.8:\n",
      "      Successfully uninstalled packaging-16.8\n",
      "Successfully installed huggingface-hub-0.4.0 packaging-21.3 regex-2022.10.31 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\users\\aisha\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import islice\n",
    "#from transformers import AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha_df_path = r'semeval2023-afrisenti-main/data/raw/train/ha_train.tsv'\n",
    "ha_df = pd.read_csv(ha_df_path, index_col=None, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ha_train_00001</td>\n",
       "      <td>@user Da kudin da Arewa babu wani abin azo aga...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ha_train_00002</td>\n",
       "      <td>@user Kaga wani Adu ar BandaðŸ’”ðŸ˜­ wai a haka Shi ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ha_train_00003</td>\n",
       "      <td>@user Sai haquri fa yan madrid daman kunce cha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ha_train_00004</td>\n",
       "      <td>@user Hmmm yanzu kai kasan girman allah daxaka...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha_train_00005</td>\n",
       "      <td>@user @user Wai gwamno nin Nigeria suna afa kw...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                              tweet     label\n",
       "0  ha_train_00001  @user Da kudin da Arewa babu wani abin azo aga...  negative\n",
       "1  ha_train_00002  @user Kaga wani Adu ar BandaðŸ’”ðŸ˜­ wai a haka Shi ...  negative\n",
       "2  ha_train_00003  @user Sai haquri fa yan madrid daman kunce cha...  negative\n",
       "3  ha_train_00004  @user Hmmm yanzu kai kasan girman allah daxaka...  negative\n",
       "4  ha_train_00005  @user @user Wai gwamno nin Nigeria suna afa kw...  negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS TO REMOVE EMOJIS / PUNCTUATION CHARACTERS / WORDS WITH LENGTH LESS THAN 3 / BANNED WORDS:\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "def remove_puncs(item):\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890â€¹â€œâ€¦â€˜â€”â€™â€'''\n",
    "    for char in item:\n",
    "        if char in punc:\n",
    "            item = item.replace(char, \"\")\n",
    "    return item\n",
    "\n",
    "#re.sub(r'\\b\\w{1,3}\\b', '', c)\n",
    "\n",
    "def remove_less_three_char(item):\n",
    "    if len(item) >= 3:\n",
    "        return item\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "banned = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', \n",
    "          'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most',\n",
    "          'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until',\n",
    "          'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', \n",
    "          'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', \n",
    "          'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', \n",
    "          'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself',\n",
    "          'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', \n",
    "          'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than', 'user']\n",
    "# banned = ['@user', '@user:'] # add stopped words here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing \n",
    "\n",
    "def preprocessing(df):\n",
    "\n",
    "    df_translated_combined_new = df #df_translated_combined #df_raw_combined\n",
    "\n",
    "    #Create list\n",
    "    df_translated_combined_new['words'] = df_translated_combined_new.tweet.str.lower().str.split()\n",
    "    # remove emojis\n",
    "    df_translated_combined_new[\"words\"] = [[remove_emojis(item) for item in x] for x in df_translated_combined_new[\"words\"]]\n",
    "    # remove punctuations\n",
    "    df_translated_combined_new[\"words\"] = [[remove_puncs(item) for item in x] for x in df_translated_combined_new[\"words\"]]\n",
    "    # remove words from banned list (including user + stopwords)\n",
    "    df_translated_combined_new[\"words\"] = [[item for item in x if item not in banned] for x in df_translated_combined_new[\"words\"]]\n",
    "    # remove stopwords\n",
    "    # df_translated_combined_new[\"words\"] = [[item for item in x if item not in stopwords.words('english')] for x in df_translated_combined_new[\"words\"]]\n",
    "    # remove words shorter than length 3\n",
    "    df_translated_combined_new[\"words\"] = [[remove_less_three_char(item) for item in x] for x in df_translated_combined_new[\"words\"]]\n",
    "    # remove empty strings from list\n",
    "    df_translated_combined_new[\"words\"] = [[item for item in x if item] for x in df_translated_combined_new[\"words\"]]\n",
    "    \n",
    "    return df_translated_combined_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ha_train_00001</td>\n",
       "      <td>@user Da kudin da Arewa babu wani abin azo aga...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kudin, arewa, babu, wani, abin, azo, agani, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ha_train_00002</td>\n",
       "      <td>@user Kaga wani Adu ar BandaðŸ’”ðŸ˜­ wai a haka Shi ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kaga, wani, adu, banda, wai, haka, shi, shuga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ha_train_00003</td>\n",
       "      <td>@user Sai haquri fa yan madrid daman kunce cha...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[sai, haquri, yan, madrid, daman, kunce, champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ha_train_00004</td>\n",
       "      <td>@user Hmmm yanzu kai kasan girman allah daxaka...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[hmmm, yanzu, kai, kasan, girman, allah, daxak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha_train_00005</td>\n",
       "      <td>@user @user Wai gwamno nin Nigeria suna afa kw...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[wai, gwamno, nin, nigeria, suna, afa, kwayoyi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14167</th>\n",
       "      <td>ha_train_14168</td>\n",
       "      <td>@user Ba wasa a fuskokinsu, may you succeed al...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wasa, fuskokinsu, may, succeed, always, guys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14168</th>\n",
       "      <td>ha_train_14169</td>\n",
       "      <td>@user Allah yasa muyi kyakkyawan qarshe ðŸ¤²ðŸ¤²</td>\n",
       "      <td>positive</td>\n",
       "      <td>[allah, yasa, muyi, kyakkyawan, qarshe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14169</th>\n",
       "      <td>ha_train_14170</td>\n",
       "      <td>@user Abu nafarko gamawa da duniya lafiya kuma...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[abu, nafarko, gamawa, duniya, lafiya, kuma, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14170</th>\n",
       "      <td>ha_train_14171</td>\n",
       "      <td>@user @user @user Allah ubangiji yaiwa rayuwar...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[allah, ubangiji, yaiwa, rayuwarsa, albarka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14171</th>\n",
       "      <td>ha_train_14172</td>\n",
       "      <td>@user @user Baka film din banza director Allah...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[baka, film, din, banza, director, allah, dafa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14172 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                              tweet  \\\n",
       "0      ha_train_00001  @user Da kudin da Arewa babu wani abin azo aga...   \n",
       "1      ha_train_00002  @user Kaga wani Adu ar BandaðŸ’”ðŸ˜­ wai a haka Shi ...   \n",
       "2      ha_train_00003  @user Sai haquri fa yan madrid daman kunce cha...   \n",
       "3      ha_train_00004  @user Hmmm yanzu kai kasan girman allah daxaka...   \n",
       "4      ha_train_00005  @user @user Wai gwamno nin Nigeria suna afa kw...   \n",
       "...               ...                                                ...   \n",
       "14167  ha_train_14168  @user Ba wasa a fuskokinsu, may you succeed al...   \n",
       "14168  ha_train_14169         @user Allah yasa muyi kyakkyawan qarshe ðŸ¤²ðŸ¤²   \n",
       "14169  ha_train_14170  @user Abu nafarko gamawa da duniya lafiya kuma...   \n",
       "14170  ha_train_14171  @user @user @user Allah ubangiji yaiwa rayuwar...   \n",
       "14171  ha_train_14172  @user @user Baka film din banza director Allah...   \n",
       "\n",
       "          label                                              words  \n",
       "0      negative  [kudin, arewa, babu, wani, abin, azo, agani, y...  \n",
       "1      negative  [kaga, wani, adu, banda, wai, haka, shi, shuga...  \n",
       "2      negative  [sai, haquri, yan, madrid, daman, kunce, champ...  \n",
       "3      negative  [hmmm, yanzu, kai, kasan, girman, allah, daxak...  \n",
       "4      negative    [wai, gwamno, nin, nigeria, suna, afa, kwayoyi]  \n",
       "...         ...                                                ...  \n",
       "14167  positive     [wasa, fuskokinsu, may, succeed, always, guys]  \n",
       "14168  positive            [allah, yasa, muyi, kyakkyawan, qarshe]  \n",
       "14169  positive  [abu, nafarko, gamawa, duniya, lafiya, kuma, i...  \n",
       "14170  positive       [allah, ubangiji, yaiwa, rayuwarsa, albarka]  \n",
       "14171  positive  [baka, film, din, banza, director, allah, dafa...  \n",
       "\n",
       "[14172 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha_df = preprocessing(ha_df)\n",
    "ha_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ha_train_00001</td>\n",
       "      <td>@user Da kudin da Arewa babu wani abin azo aga...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kudin, arewa, babu, wani, abin, azo, agani, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ha_train_00002</td>\n",
       "      <td>@user Kaga wani Adu ar BandaðŸ’”ðŸ˜­ wai a haka Shi ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kaga, wani, adu, banda, wai, haka, shi, shuga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ha_train_00003</td>\n",
       "      <td>@user Sai haquri fa yan madrid daman kunce cha...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[sai, haquri, yan, madrid, daman, kunce, champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ha_train_00004</td>\n",
       "      <td>@user Hmmm yanzu kai kasan girman allah daxaka...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[hmmm, yanzu, kai, kasan, girman, allah, daxak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha_train_00005</td>\n",
       "      <td>@user @user Wai gwamno nin Nigeria suna afa kw...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[wai, gwamno, nin, nigeria, suna, afa, kwayoyi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14167</th>\n",
       "      <td>ha_train_14168</td>\n",
       "      <td>@user Ba wasa a fuskokinsu, may you succeed al...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wasa, fuskokinsu, may, succeed, always, guys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14168</th>\n",
       "      <td>ha_train_14169</td>\n",
       "      <td>@user Allah yasa muyi kyakkyawan qarshe ðŸ¤²ðŸ¤²</td>\n",
       "      <td>positive</td>\n",
       "      <td>[allah, yasa, muyi, kyakkyawan, qarshe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14169</th>\n",
       "      <td>ha_train_14170</td>\n",
       "      <td>@user Abu nafarko gamawa da duniya lafiya kuma...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[abu, nafarko, gamawa, duniya, lafiya, kuma, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14170</th>\n",
       "      <td>ha_train_14171</td>\n",
       "      <td>@user @user @user Allah ubangiji yaiwa rayuwar...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[allah, ubangiji, yaiwa, rayuwarsa, albarka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14171</th>\n",
       "      <td>ha_train_14172</td>\n",
       "      <td>@user @user Baka film din banza director Allah...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[baka, film, din, banza, director, allah, dafa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14172 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               text  \\\n",
       "0      ha_train_00001  @user Da kudin da Arewa babu wani abin azo aga...   \n",
       "1      ha_train_00002  @user Kaga wani Adu ar BandaðŸ’”ðŸ˜­ wai a haka Shi ...   \n",
       "2      ha_train_00003  @user Sai haquri fa yan madrid daman kunce cha...   \n",
       "3      ha_train_00004  @user Hmmm yanzu kai kasan girman allah daxaka...   \n",
       "4      ha_train_00005  @user @user Wai gwamno nin Nigeria suna afa kw...   \n",
       "...               ...                                                ...   \n",
       "14167  ha_train_14168  @user Ba wasa a fuskokinsu, may you succeed al...   \n",
       "14168  ha_train_14169         @user Allah yasa muyi kyakkyawan qarshe ðŸ¤²ðŸ¤²   \n",
       "14169  ha_train_14170  @user Abu nafarko gamawa da duniya lafiya kuma...   \n",
       "14170  ha_train_14171  @user @user @user Allah ubangiji yaiwa rayuwar...   \n",
       "14171  ha_train_14172  @user @user Baka film din banza director Allah...   \n",
       "\n",
       "          label                                              words  \n",
       "0      negative  [kudin, arewa, babu, wani, abin, azo, agani, y...  \n",
       "1      negative  [kaga, wani, adu, banda, wai, haka, shi, shuga...  \n",
       "2      negative  [sai, haquri, yan, madrid, daman, kunce, champ...  \n",
       "3      negative  [hmmm, yanzu, kai, kasan, girman, allah, daxak...  \n",
       "4      negative    [wai, gwamno, nin, nigeria, suna, afa, kwayoyi]  \n",
       "...         ...                                                ...  \n",
       "14167  positive     [wasa, fuskokinsu, may, succeed, always, guys]  \n",
       "14168  positive            [allah, yasa, muyi, kyakkyawan, qarshe]  \n",
       "14169  positive  [abu, nafarko, gamawa, duniya, lafiya, kuma, i...  \n",
       "14170  positive       [allah, ubangiji, yaiwa, rayuwarsa, albarka]  \n",
       "14171  positive  [baka, film, din, banza, director, allah, dafa...  \n",
       "\n",
       "[14172 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha_df = ha_df.rename(columns={'tweet': 'text'})\n",
    "ha_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'semeval2023-afrisenti-main/data/raw/train/ha/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = ha_df_path.rsplit('/', 1)\n",
    "train_data_path = train_data_path[0] + \"/ha/\"\n",
    "train_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to a file\n",
    "ha_df.to_csv(train_data_path + 'ha_train.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform analysis on ha using LinearSVC, Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39482"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Number of unique words in the given data.\n",
    "# len(set(\" \".join(df_am_filtered[\"text\"]).split()))\n",
    "\n",
    "len(set(\" \".join(ha_df[\"text\"]).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 35000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_labels(tsv_path):\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    data = df['text'].to_list()\n",
    "    labels = df['label'].to_list()\n",
    "    labels = [t.strip().upper() for t in labels]\n",
    "    return data, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "def get_features(corpus, vocab_size):\n",
    "    vectorizer = CountVectorizer(    ## for multinomial NB\n",
    "    #vectorizer = TfidfVectorizer(  ## for linear SVC\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=vocab_size,\n",
    "    tokenizer=identity_tokenizer, # already receiving tokenized text from AUtotokenizer\n",
    "    lowercase=False,\n",
    "    token_pattern=None\n",
    "    )\n",
    "    vectorizer.fit(corpus)\n",
    "    X = vectorizer.transform(corpus)\n",
    "    return X, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kflod(data, true_labels, tokenizer):\n",
    "    skf = StratifiedKFold(n_splits=5)   #10\n",
    "    tokenized_texts_str = [tokenizer.convert_ids_to_tokens(text) for text in tokenizer(data)['input_ids']]\n",
    "    features,_ = get_features(tokenized_texts_str, VOCAB_SIZE)\n",
    "    true_test_labels = []\n",
    "    predicted_test_labels = []\n",
    "    for train_index, test_index in skf.split(features, true_labels):\n",
    "        # classifier = LinearSVC(C=0.2,max_iter=1000)  #0.2, C=100,max_iter=10000\n",
    "        classifier = MultinomialNB()\n",
    "        train_labels = true_labels[train_index]\n",
    "        classifier.fit(features[train_index], train_labels)\n",
    "        train_predictions = classifier.predict(features[train_index])\n",
    "        test_predictions = classifier.predict(features[test_index])\n",
    "        test_labels = true_labels[test_index]\n",
    "        true_test_labels.extend(test_labels)\n",
    "        predicted_test_labels.extend(test_predictions)\n",
    "        # train_f1 = f1_score(train_labels, train_predictions, average='micro', labels=['NEGATIVE', 'NEUTRAL','POSITIVE'])\n",
    "        # test_f1 = f1_score(test_labels, test_predictions, average='micro', labels=['NEGATIVE', 'NEUTRAL','POSITIVE'])\n",
    "        # print(f\"Train F1: {train_f1}, Test F1: {test_f1}\")\n",
    "    return true_test_labels, predicted_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_and_dataset(data_set_path):\n",
    "    data_files = sorted(Path(data_set_path).glob('*.tsv'))\n",
    "    african_language_model = 'Davlan/afro-xlmr-mini'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(african_language_model)\n",
    "    results = []\n",
    "    \n",
    "    true_labels_details  = []\n",
    "    predicted_labels_details  = []\n",
    "    \n",
    "    for csv_file in tqdm(data_files, total=len(data_files)):\n",
    "        language = csv_file.stem.split('_')[0]\n",
    "        model = \"Multinomial Naive Bayes\"\n",
    "        # model = \"Linear SVC\"\n",
    "\n",
    "        data, true_labels = get_data_and_labels(csv_file)\n",
    "        label_set = ['NEGATIVE', 'POSITIVE', 'NEUTRAL']\n",
    "        true_labels, predicted_labels = evaluate_kflod(data, true_labels, tokenizer)\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        precision = precision_score(true_labels, predicted_labels, labels=label_set, average=None)\n",
    "        recall = recall_score(true_labels, predicted_labels, labels=label_set, average=None)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "        \n",
    "        true_labels_details.append(true_labels)\n",
    "        predicted_labels_details.append(predicted_labels)\n",
    "        \n",
    "        output = {  'model': model,\n",
    "                    'language': language,\n",
    "                    'num_examples': len(data),\n",
    "                    'precision': {label_set[i]: precision[i] for i in range(len(label_set))},\n",
    "                    'recall': {label_set[i]: recall[i] for i in range(len(label_set))},\n",
    "                    'f1_score_macro': f1,\n",
    "                    'accuracy': accuracy\n",
    "                }\n",
    "        results.append(output)\n",
    "    return results, true_labels_details, predicted_labels_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval2023-afrisenti-main/data/raw/train/ha/\n"
     ]
    }
   ],
   "source": [
    "# train_data_path = ha_df_path.rsplit('/', 1)\n",
    "# train_data_path = train_data_path[0] + \"/ha/\"\n",
    "# print(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:16<00:00, 16.32s/it]\n"
     ]
    }
   ],
   "source": [
    "#ha_df_path =  r'semeval2023-afrisenti-main/data/raw/train/ha_train.tsv'\n",
    "\n",
    "results, tr_labels, pr_labels = evaluate_model_and_dataset(train_data_path)\n",
    "\n",
    "# df = pd.DataFrame(results)\n",
    "# df.to_excel('../reports/linear_svm_evaluation.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>language</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>ha</td>\n",
       "      <td>14172</td>\n",
       "      <td>{'NEGATIVE': 0.7236648701616855, 'POSITIVE': 0.8634694814134924, 'NEUTRAL': 0.6615491974877878}</td>\n",
       "      <td>{'NEGATIVE': 0.6459654493767767, 'POSITIVE': 0.8028589716236398, 'NEUTRAL': 0.7719869706840391}</td>\n",
       "      <td>0.742396</td>\n",
       "      <td>0.741533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model language  num_examples  \\\n",
       "0  Linear SVC       ha         14172   \n",
       "\n",
       "                                                                                         precision  \\\n",
       "0  {'NEGATIVE': 0.7236648701616855, 'POSITIVE': 0.8634694814134924, 'NEUTRAL': 0.6615491974877878}   \n",
       "\n",
       "                                                                                            recall  \\\n",
       "0  {'NEGATIVE': 0.6459654493767767, 'POSITIVE': 0.8028589716236398, 'NEUTRAL': 0.7719869706840391}   \n",
       "\n",
       "   f1_score_macro  accuracy  \n",
       "0        0.742396  0.741533  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## linearSVC, cv = 10, C=0.2 \n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>language</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>ha</td>\n",
       "      <td>14172</td>\n",
       "      <td>{'NEGATIVE': 0.7141802067946824, 'POSITIVE': 0.8527860582435222, 'NEUTRAL': 0.6505479213776308}</td>\n",
       "      <td>{'NEGATIVE': 0.6343756833588454, 'POSITIVE': 0.7934713036057179, 'NEUTRAL': 0.761400651465798}</td>\n",
       "      <td>0.731866</td>\n",
       "      <td>0.731019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model language  num_examples  \\\n",
       "0  Linear SVC       ha         14172   \n",
       "\n",
       "                                                                                         precision  \\\n",
       "0  {'NEGATIVE': 0.7141802067946824, 'POSITIVE': 0.8527860582435222, 'NEUTRAL': 0.6505479213776308}   \n",
       "\n",
       "                                                                                           recall  \\\n",
       "0  {'NEGATIVE': 0.6343756833588454, 'POSITIVE': 0.7934713036057179, 'NEUTRAL': 0.761400651465798}   \n",
       "\n",
       "   f1_score_macro  accuracy  \n",
       "0        0.731866  0.731019  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## linearSVC, cv =3, C=0.2\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>language</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>ha</td>\n",
       "      <td>14172</td>\n",
       "      <td>{'NEGATIVE': 0.7051551201306275, 'POSITIVE': 0.8503370786516854, 'NEUTRAL': 0.6675252989880405}</td>\n",
       "      <td>{'NEGATIVE': 0.6610540126831401, 'POSITIVE': 0.8073394495412844, 'NEUTRAL': 0.738599348534202}</td>\n",
       "      <td>0.737313</td>\n",
       "      <td>0.736311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model language  num_examples  \\\n",
       "0  Linear SVC       ha         14172   \n",
       "\n",
       "                                                                                         precision  \\\n",
       "0  {'NEGATIVE': 0.7051551201306275, 'POSITIVE': 0.8503370786516854, 'NEUTRAL': 0.6675252989880405}   \n",
       "\n",
       "                                                                                           recall  \\\n",
       "0  {'NEGATIVE': 0.6610540126831401, 'POSITIVE': 0.8073394495412844, 'NEUTRAL': 0.738599348534202}   \n",
       "\n",
       "   f1_score_macro  accuracy  \n",
       "0        0.737313  0.736311  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## linearSVC, cv =3, C=1\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>language</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>ha</td>\n",
       "      <td>14172</td>\n",
       "      <td>{'NEGATIVE': 0.6921842768737108, 'POSITIVE': 0.8362508309328607, 'NEUTRAL': 0.6648413897280967}</td>\n",
       "      <td>{'NEGATIVE': 0.6603979881915591, 'POSITIVE': 0.8052058886281204, 'NEUTRAL': 0.7168159609120521}</td>\n",
       "      <td>0.728735</td>\n",
       "      <td>0.727844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model language  num_examples  \\\n",
       "0  Linear SVC       ha         14172   \n",
       "\n",
       "                                                                                         precision  \\\n",
       "0  {'NEGATIVE': 0.6921842768737108, 'POSITIVE': 0.8362508309328607, 'NEUTRAL': 0.6648413897280967}   \n",
       "\n",
       "                                                                                            recall  \\\n",
       "0  {'NEGATIVE': 0.6603979881915591, 'POSITIVE': 0.8052058886281204, 'NEUTRAL': 0.7168159609120521}   \n",
       "\n",
       "   f1_score_macro  accuracy  \n",
       "0        0.728735  0.727844  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## linearSVC, cv =3, C=5\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>language</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>ha</td>\n",
       "      <td>14172</td>\n",
       "      <td>{'NEGATIVE': 0.6953996282527881, 'POSITIVE': 0.8612167300380228, 'NEUTRAL': 0.6469964664310954}</td>\n",
       "      <td>{'NEGATIVE': 0.6544937677673299, 'POSITIVE': 0.7732024749306593, 'NEUTRAL': 0.7455211726384365}</td>\n",
       "      <td>0.727313</td>\n",
       "      <td>0.725303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model language  num_examples  \\\n",
       "0  Multinomial Naive Bayes       ha         14172   \n",
       "\n",
       "                                                                                         precision  \\\n",
       "0  {'NEGATIVE': 0.6953996282527881, 'POSITIVE': 0.8612167300380228, 'NEUTRAL': 0.6469964664310954}   \n",
       "\n",
       "                                                                                            recall  \\\n",
       "0  {'NEGATIVE': 0.6544937677673299, 'POSITIVE': 0.7732024749306593, 'NEUTRAL': 0.7455211726384365}   \n",
       "\n",
       "   f1_score_macro  accuracy  \n",
       "0        0.727313  0.725303  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multinomial Naive Bayes, cv = 10 \n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>language</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>ha</td>\n",
       "      <td>14172</td>\n",
       "      <td>{'NEGATIVE': 0.6908335257446317, 'POSITIVE': 0.8580246913580247, 'NEUTRAL': 0.6438088470421034}</td>\n",
       "      <td>{'NEGATIVE': 0.654275092936803, 'POSITIVE': 0.7710689140174952, 'NEUTRAL': 0.737785016286645}</td>\n",
       "      <td>0.723961</td>\n",
       "      <td>0.721846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model language  num_examples  \\\n",
       "0  Multinomial Naive Bayes       ha         14172   \n",
       "\n",
       "                                                                                         precision  \\\n",
       "0  {'NEGATIVE': 0.6908335257446317, 'POSITIVE': 0.8580246913580247, 'NEUTRAL': 0.6438088470421034}   \n",
       "\n",
       "                                                                                          recall  \\\n",
       "0  {'NEGATIVE': 0.654275092936803, 'POSITIVE': 0.7710689140174952, 'NEUTRAL': 0.737785016286645}   \n",
       "\n",
       "   f1_score_macro  accuracy  \n",
       "0        0.723961  0.721846  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multinomial Naive Bayes, cv = 5\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
